# LangChain Prompts

## 📌 Definition

A **prompt** is an instruction or query given to a language model (LLM) that guides its response.  
Prompts enable natural language interaction with AI, reducing the need for direct coding.

---

## 🧩 Prompt Templates

**Purpose:** Structure prompts to make them reusable and adaptable.  
👉 Instead of hardcoding, templates use **placeholders** for dynamic values and context.

### 🔹 Types

1. **Text Prompt Templates**

   - Input: A string of text
   - Output: A formatted prompt

2. **Chat Prompt Templates**
   - Input: List of messages (role-based: _System, Human, AI_)
   - Each message can include variables

### 🔹 Variables

- Use placeholders like `{input}` or `{context}`
- Replaced with user-provided or system-generated data at runtime

---

## 🎯 Prompt Engineering

- **Zero-shot Prompts:** Rely only on the task/question and LLM’s training.
- **Few-shot Prompts:** Include a few examples in the prompt for more accurate/specific responses.
- **Dynamic & Reusable Prompts:**
  - Example: Summarization prompts with adjustable tone or topic.
  - Reuse across apps for consistency.

---

## 🛠️ Managing Prompts in LangChain

- **Structure:** Keep prompt logic separate from application code.
- **Versioning:** Different prompt versions for dev/test/production.
- **Composability:** Combine prompts so one output becomes another’s input (pipeline workflows).

---

## 💻 Example Code

```python
from langchain.prompts import PromptTemplate

# Define a simple template
template = PromptTemplate(
    input_variables=["name", "task"],
    template="Hello {name}, could you please help me with {task}?"
)

prompt = template.format(name="Alice", task="data analysis")
print(prompt)
# Output: Hello Alice, could you please help me with data analysis?
```
